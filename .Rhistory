# załadowanie bibliotek
library(tm)
corpus_dir <- "./Literatura - streszczenia - oryginał"
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
View(corpus)
corpus$`Harry Potter i Kamien Filozoficzny.txt`$content
corpus[[1]]$content
# wstępne przetwarzanie
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
stoplist_file <- "./stopwords_pl.txt"
stoplist <- readLines(stoplist_file, encoding = "UTF-8")
corpus <- tm_map(corpus, removeWords, stoplist)
# eksport przetworzonego korpusu do plików tekstowych
preprocessed_dir <- "./Literatura - streszczenia - przetworzone"
dir.create(preprocessed_dir)
writeCorpus(corpus, preprocessed_dir)
corpus <- tm_map(corpus, stripWhitespace)
# eksport przetworzonego korpusu do plików tekstowych
preprocessed_dir <- "./Literatura - streszczenia - przetworzone"
dir.create(preprocessed_dir)
writeCorpus(corpus, preprocessed_dir)
# załadowanie bibliotek
library(tm)
View(corpus)
View(corpus)
# utworzenie korpusu dokumentów
corpus_dir <- "./Literatura - streszczenia - oryginał"
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# wstępne przetwarzanie
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
stoplist_file <- "./stopwords_pl.txt"
stoplist <- readLines(stoplist_file, encoding = "UTF-8")
corpus <- tm_map(corpus, removeWords, stoplist)
corpus <- tm_map(corpus, stripWhitespace)
# eksport przetworzonego korpusu do plików tekstowych
preprocessed_dir <- "./Literatura - streszczenia - przetworzone"
dir.create(preprocessed_dir)
writeCorpus(corpus, preprocessed_dir)
# dodatkowe funkcje transformujące
paste_paragraphs <- function(text){
paste(text, collapse = " ")
}
remove_char <- content_transformer(
function(text, char) gsub(char, "", text)
)
cut_extensions <- function(document){
meta(document, "id") <- gsub(
"\\.txt$",
"",
meta(document, "id")
)
return(document)
}
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# wstępne przetwarzanie
corpus <- tm_map(corpus, content_transformer(paste_paragraphs))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
stoplist_file <- "./stopwords_pl.txt"
stoplist <- readLines(stoplist_file, encoding = "UTF-8")
corpus <- tm_map(corpus, removeWords, stoplist)
corpus <- tm_map(corpus, stripWhitespace)
install.packages("hunspell")
library(hunspell)
# lematyzacja
polish <- dictionary("pl_PL")
polish
lemmatize <- function(text){
parsed_text_vec <- unlist(hunspell_parse(text, dict = polish))
lemmatized_text_vec <- hunspell_stem(parsed_text_vec, polish)
for (t in 1:length(lemmatized_text_vec)) {
if(length(lemmatized_text_vec[[t]]) == 0)
lemmatized_text_vec[t] <- parsed_text_vec[t]
if(length(lemmatized_text_vec[[t]])  > 1)
lemmatized_text_vec[t] <- lemmatized_text_vec[[t]][1]
lemmatized_text <- paste(lemmatized_text_vec, collapse = " ")
return(lemmatized_text)
}
}
corpus <- tm_map(corpus, content_transformer(lemmatize))
corpus <- tm_map(corpus, cut_extensions)
# eksport przetworzonego korpusu do plików tekstowych
preprocessed_dir <- "./Literatura - streszczenia - przetworzone"
dir.create(preprocessed_dir)
writeCorpus(corpus, preprocessed_dir)
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# wstępne przetwarzanie
corpus <- tm_map(corpus, content_transformer(paste_paragraphs))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
stoplist_file <- "./stopwords_pl.txt"
stoplist <- readLines(stoplist_file, encoding = "UTF-8")
corpus <- tm_map(corpus, removeWords, stoplist)
corpus <- tm_map(corpus, remove_char, intToUtf8(8722))
corpus <- tm_map(corpus, remove_char, intToUtf8(190))
corpus <- tm_map(corpus, stripWhitespace)
# lematyzacja
polish <- dictionary("pl_PL")
lemmatize <- function(text){
parsed_text_vec <- unlist(hunspell_parse(text, dict = polish))
lemmatized_text_vec <- hunspell_stem(parsed_text_vec, polish)
for (t in 1:length(lemmatized_text_vec)) {
if(length(lemmatized_text_vec[[t]]) == 0) {
lemmatized_text_vec[t] <- parsed_text_vec[t]
}
if(length(lemmatized_text_vec[[t]])  > 1) {
lemmatized_text_vec[t] <- lemmatized_text_vec[[t]][1]
}
lemmatized_text <- paste(lemmatized_text_vec, collapse = " ")
return(lemmatized_text)
}
}
corpus <- tm_map(corpus, content_transformer(lemmatize))
corpus <- tm_map(corpus, cut_extensions)
# eksport przetworzonego korpusu do plików tekstowych
preprocessed_dir <- "./Literatura - streszczenia - przetworzone"
dir.create(preprocessed_dir)
writeCorpus(corpus, preprocessed_dir)
lemmatize <- function(text){
parsed_text_vec <- unlist(hunspell_parse(text, dict = polish))
lemmatized_text_vec <- hunspell_stem(parsed_text_vec, polish)
for (t in 1:length(lemmatized_text_vec)) {
if(length(lemmatized_text_vec[[t]]) == 0) {
lemmatized_text_vec[t] <- parsed_text_vec[t]
}
if(length(lemmatized_text_vec[[t]])  > 1) {
lemmatized_text_vec[t] <- lemmatized_text_vec[[t]][1]
}
lemmatized_text <- paste(lemmatized_text_vec, collapse = " ")
}
return(lemmatized_text)
}
corpus <- VCorpus(
DirSource(
corpus_dir,
"UTF-8",
"*.txt"
),
readerControl = list(
language = "pl_PL"
)
)
# wstępne przetwarzanie
corpus <- tm_map(corpus, content_transformer(paste_paragraphs))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
stoplist_file <- "./stopwords_pl.txt"
stoplist <- readLines(stoplist_file, encoding = "UTF-8")
corpus <- tm_map(corpus, removeWords, stoplist)
corpus <- tm_map(corpus, remove_char, intToUtf8(8722))
corpus <- tm_map(corpus, remove_char, intToUtf8(190))
corpus <- tm_map(corpus, stripWhitespace)
lemmatize <- function(text){
parsed_text_vec <- unlist(hunspell_parse(text, dict = polish))
lemmatized_text_vec <- hunspell_stem(parsed_text_vec, polish)
for (t in 1:length(lemmatized_text_vec)) {
if(length(lemmatized_text_vec[[t]]) == 0) {
lemmatized_text_vec[t] <- parsed_text_vec[t]
}
if(length(lemmatized_text_vec[[t]])  > 1) {
lemmatized_text_vec[t] <- lemmatized_text_vec[[t]][1]
}
lemmatized_text <- paste(lemmatized_text_vec, collapse = " ")
}
return(lemmatized_text)
}
corpus <- tm_map(corpus, content_transformer(lemmatize))
corpus <- tm_map(corpus, cut_extensions)
# eksport przetworzonego korpusu do plików tekstowych
preprocessed_dir <- "./Literatura - streszczenia - przetworzone"
dir.create(preprocessed_dir)
writeCorpus(corpus, preprocessed_dir)
# utworzenie macierzzy częstości
tdm_tf_all <- TermDocumentMatrix(corpus)
dtm_tf_all <- DocumentTermMatrix(corpus)
tdm_tfidf_all <- TermDocumentMatrix(
corpus,
control = list(
weigthing = weightTfIdf
)
)
tdm_tf_bounds <- TermDocumentMatrix(
corpus,
control = list(
bounds = list(
global = c(2,16)
)
)
)
tdm_tfidf_bounds <- TermDocumentMatrix(
corpus,
control = list(
weigthing = weightTfIdf,
bounds = list(
global = c(2,16)
)
)
)
tdm_tf_all_m <- as.matrix(tdm_tf_all)
View(tdm_tf_all_m)
tdm_tf_all_df <- as.data.frame(tdm_tf_all_m)
View(tdm_tf_all_df)
matrix_file <- "./tdm_tf_all.csv"
write.table(
tdm_tf_all_m,
matrix_file,
sep = ";",
dec = ",",
col.names = NA
)
